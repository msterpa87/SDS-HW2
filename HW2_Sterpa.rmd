---
title: "HW2_Sterpa"
author: "Marco Aurelio Sterpa"
date: "26/12/2020"
output: html_document
---

```{r helper functions, message=FALSE, warning=FALSE}
library(mixtools)
library(caret)
library(KScorrect)
library(gt)
library(plyr)

# helper functions used to compute the vectorized (log)likelihood
vnorm <- function(y, p, mu, sigma) p * dnorm(y, mu, sigma)
vect_wnorm <- Vectorize(vnorm, c("p", "mu", "sigma"))
log_vnorm <- function(y, p, mu, sigma) p * dnorm(y, mu, sigma, log = TRUE)
vect_log_wnorm <- Vectorize(vnorm, c("p", "mu", "sigma"))
vect_norm = Vectorize(dnorm, c("mean", "sd"))
```

In order to study the performance of different model selection methods on the EM-fit of the Bart Simpson distribution we will first need a procedure to create a sample from this density.

```{r}
# create a random sample
sample_sim <- function(n) rnormmix(n,
                                   lambda = c(0.5, rep(0.1,5)),
                                   mu     = c(0, ((0:4)/2)-1),
                                   sigma  = c(1, rep(0.1,5)))
```

We will use two sample of size 100 and 1000 showing respectively a non-asymptotic and asymptotic behaviour. Here's a clearly non asymptotic sample:

```{r}
small_sample <- sample_sim(100)
hist(small_sample,
     main = "Sample data with non-asymptotic behaviour",
     xlab = "n = 100",
     breaks = 50)
```

And here's a sample that clearly looks more much more like the Bart:

```{r}
large_sample <- sample_sim(1000)
hist(large_sample,
     main = "Sample data with asymptotic behaviour",
     xlab = "n = 1000",
     breaks = 100)
```


We start by defining an EM-fit procedure for a mixture of a generic number of components and a simple function to compute the loglikelihood given the output of the EM-fit.

```{r}
# Handmade EM4MoG ---------------------------------------------------------
handmade.em <- function(y, p, mu, sigma, breaks = 50, n_iter = 1000,
                        threshold = 1e-3, plot = F, plot_freq = 25)
{
  # vectorized likelihood
  like     <- apply(vect_wnorm(y, p, mu, sigma), 1, sum)
  deviance <- -2 * sum(log(like))
  k <- length(p)
  
  # creating matrix of iteration values
  n_col    <- 2 + 3 * k
  res      <- matrix(NA, n_iter + 1, n_col)
  res[1,]  <- c(0, p, mu, sigma, deviance)
  
  for (iter in 1:n_iter) {
    # E step
    d   <- vect_wnorm(y, p, mu, sigma) # matrix of responsabilities
    den <- apply(d, 1, sum) # normalizing factor
    r <- d / den # normalizing each column
    r[is.na(r)] = 1

    # M step
    p <- apply(r, 2, mean)
    mu <- apply(r, 2, function(x) sum(x * y) / sum(x))
    #for (i in 1:k) {
    #  mu[i] <- sum(r[,i] * y) / sum(r[,i])
    #}

    for (i in 1:k) sigma[i] <- sqrt(sum(r[,i] * (y - mu[i])^2) / sum(r[,i]))
    #sigma <- apply(1:k, 2, function(i) sqrt(sum(r[,i] * (y - mu[i])^2) / sum(r[,i]) ) )
    #sigma <- sqrt(sigma - mu^2)
    
    # -2 x log-likelihood (a.k.a. deviance)
    new_like <- log(apply(vect_wnorm(y, p, mu, sigma), 1, sum))
    
    # stopping criteria
    diff <- abs(sum(like) - sum(new_like))
    if (!is.na(diff) && diff < threshold) break
    like     <- new_like
    deviance <- -2 * sum(like)
    
    # Save
    res[iter+1,] <- c(iter, p, mu, sigma, deviance)
    
    # Plot
    if (plot && iter %% plot_freq == 0) {
      xlab <- paste("EM Iteration: ", iter, "/", n_iter, sep = "")
      plot_fit(y, p, mu, sigma, xlab)
      Sys.sleep(1)
    }
  }
  res <- data.frame(res)
  
  # function to generate strings
  append_str <- function(str) sapply(1:k, function(x) paste(str, x, sep=''))
  p_names <- append_str("p")
  mu_names <- append_str("mu")
  sigma_names <- append_str("sigma")
  names(res) <- c("iteration", p_names, mu_names, sigma_names, "deviance")
  out <- list(p = p, mu = mu, sigma = sigma, deviance = deviance, res = res, iter = iter)
  return(out)
}

log_likelihood <- function(x, hm.fit) {
  # x : vector
  # hm.fit : output of handmade.em function
  like <- apply(vect_wnorm(x, hm.fit$p, hm.fit$mu, hm.fit$sigma), 1, sum)
  log_like <- sum(log(like))
  return(log_like)
}

```

Next we need a function to initialize the starting parameters, a few options have been tested whose code is reported below, but ultimately the KMeans initialization has been used. The results of EM-fit using different initialization procedures have been tested by comparing the model produced with the R builtin function

```{r}
init_kmeans <- function(y, k) {
  # takes the data and the number of components and returns
  # initial parameters (proportions, mean, std) running the
  # kmeans algotithm
  labels <- kmeans(y, k)$cluster
  mu <- unlist(lapply(1:k, function(i) mean(y[labels == i])))
  sigma <- unlist(lapply(1:k, function(i) sd(y[labels == i])))
  p <- unlist(lapply(1:k, function(i) sum(labels == i) / length(labels)))
  return(list(p = p, mu = mu, sigma = sigma))
}

init_params <- function(k) {
  # takes the data and the number of components and returns random
  # initial parameters, data are used to compute min/max to
  # generate mu and sigma
  p <- runif(k, 0, 1)
  p <- p / sum(p)
  mu <- runif(k, 0, 1)
  sigma <- runif(k, 0, 1)
  return(list(p = p, mu = mu, sigma = sigma))
}

init_params_groups <- function(y, k) {
  # takes the data and the number of components and returns random
  # initial parameters, data are used to compute min/max to
  # generate mu and sigma
  folds <- createFolds(y, k) # group of indexes
  folds <- rapply(folds, function(x) y[x], how = "replace") # groups of values
  p <- rep(1 / k, k) # same proportion for each component
  p <- p / sum(p) # normalizing
  mu <- as.vector(rapply(folds, mean)) # mean of each random group
  sigma <- as.vector(rapply(folds, sd))
  return(list(p = p, mu = mu, sigma = sigma))
}

```

Some of the model selection methods that will used include sample splitting and cross validation, the following functions are helpful in making the simulation code a little more neat.

```{r}
sample_split <- function(x, ratio) {
  # x : data
  # ratio : float (represents length(train)/)
  # returns a random split of x in train and test set based on ratio
  n <- length(x)
  n_train <- as.integer(n * ratio)
  train_idx <- sample(1:n, size = n_train) # sample indexes for trainset
  test_idx <- setdiff(1:n, train_idx)
  return(list(train = x[train_idx], test = x[test_idx]))
}

kfold_to_split <- function(XX, kfold, k) {
  # kfold : is the result of createFolds() with names Fold1, Fold2, ...
  # returns a list with names train and test, setting Fold_k as test set
  # and the rest as training set
  train_fold_idxs <- setdiff(1:length(kfold), k)
  train_idxs <- unlist(kfold[train_fold_idxs], use.names = F)
  test_idxs <- kfold[[k]]
  return(list(train = XX[train_idxs], test = XX[test_idxs]))
}

kfold_cv <- function(XX, n_fold, k) {
  # x : data
  # k : number of folds
  # returns the average risk running a kfold cross validation on x
  kfold <- createFolds(XX, n_fold) # named list of Fold1, Fold2, ...
  outs <- rep(NA, n_fold)
  
  for (i in 1:n_fold) {
    x_split <- kfold_to_split(XX, kfold, i) # named list of train/test
    params <- init_kmeans(x_split$train, k)
    hm.fit <- handmade.em(x_split$train, params$p, params$mu, params$sigma)
    outs[i] <- log_likelihood(x_split$test, hm.fit) / length(x_split$test)
  }
  return(mean(outs[!is.nan(outs)]))
}
```

Finally here's the simulation function that takes the sample size, the number of simulations to run and $k_{max}$ and runs M simulation for each $k \in \{1, \dots, k_{max}\}$ saving the performance of each model selection method on the result of the EM-fit.

```{r}
EM_simulation <- function(n, M, k_max, subdivisions = 200) {
  # n : sample size
  # M : number of simulations
  # k_max : maximum number of components
  # n_iter : number of iterations of EM
  
  result <- matrix(nrow = 8, ncol = M)
  model_performance <- array(dim = c(8, k_max, M))

  for (n_sim in 1:M) {
    
    # sample from Bart
    XX <- sample_sim(n)
    
    # model evaluation matrix
    model_eval <- matrix(nrow = 8, ncol = k_max)

    for (k in 1:k_max) {
      # first run EM algorithm on the whole dataset to compute AIC and BIC
      params <- init_kmeans(XX, k) # initial random parameters
      hm.fit <- handmade.em(XX, params$p, params$mu, params$sigma)
      
      # AIC and BIC evaluation
      log_like <- log_likelihood(XX, hm.fit)
      model_eval[1,k] <- 2 * log_like - 2 * k * 3
      model_eval[2,k] <- 2 * log_like - log(n) * k * 3
      
      # cross validation on different split ratios
      ratios = c(.5, .7, .3)
      
      for (i in 1:3) {
        x_split <- sample_split(XX, ratios[i])
        params <- init_kmeans(x_split$train, k) # initial parameters
        hm.fit <- handmade.em(x_split$train, params$p, params$mu, params$sigma)
        log_like <- log_likelihood(x_split$test, hm.fit) / length(x_split$test)
        model_eval[i+2,k] <- log_like
      }
      
      # 5-fold cross validation
      model_eval[6,k] <- kfold_cv(XX, 5, k)
      
      # 10-fold cross validation
      model_eval[7,k] <- kfold_cv(XX, 10, k)
      
      # Wasserstein score
      x_split <- sample_split(XX, .5) # split data in half
      params <- init_kmeans(x_split$train, k)
      hm.fit <- handmade.em(x_split$train, params$p, params$mu, params$sigma) # get MLE from trainset
      f_k <- function(p) qmixnorm(p, mean = hm.fit$mu,
                                  sd = hm.fit$sigma,
                                  pro = hm.fit$p,
                                  expand = .2) # quantile function on the MLE
      test_ecdf <- ecdf(x_split$test) # ecdf of the test set
      f_test <- function(p) as.numeric(quantile(test_ecdf, probs = p)) # quantile function of testset ecdf
      f_abs <- function(p) abs(f_k(p) - f_test(p)) # integrand
      model_eval[8,k] <- integrate(f_abs, lower = 0, upper = 1,
                                   subdivisions = subdivisions,
                                   stop.on.error = FALSE)$value
    }

    # take best k for each mode evaluation and save it in the result matrix
    model_eval[is.nan(model_eval)] = -Inf # eliminating possible NaN values
    model_performance[,,n_sim] = model_eval
    result[,n_sim] <- tryCatch(unlist(apply(model_eval, 1, which.max)))  # select the best k for each simulation
  }
  
  return(list("best_k" = result, "performance" = model_performance))
}

```

As outlined above we'll run the test on two different sample sizes, collect the results and comment on them.
(In the following chunk large and small.result have been mistankely inverted, i.e. small.result is the one on the large sample)

```{r simulations, echo=TRUE, warning=FALSE, cache=TRUE}
n_1 <- 100
n_2 <- 1000
M <- 100
k_max <- 8
small.result <- EM_simulation(n_1, M, k_max)
large.result <- EM_simulation(n_2, M, k_max)
```


We can now look at the accuracy of each model in predicting the real value $k^*=6$.

```{r}
accuracy <- function(k_matrix, k) {
  result <- unlist(apply(k_matrix, 1, function(x) sum(x == k) / length(x)))
  return(result)
}

make.accuracy.table <- function(k_matrix, k_max) {
  accuracy.table <- ldply(1:k_max, function(i) accuracy(k_matrix, i))
  colnames(accuracy.table) <- model.names
  k <- 1:k_max
  return(cbind(k, accuracy.table))
}

# table generation
show.table <- function(table, title) {
  max.rows <- apply(table, 2, which.max)[2:9]
  cells <- lapply(model.names, function(x) cells_body(columns = x, rows = max.rows[[x]]))
  
  table %>% gt() %>%
    tab_header(
      title = title
    ) %>%
    fmt_percent(
      columns = 1:k_max+1,
      decimals = 0
    ) %>%
    tab_style(
      style = list(
        cell_text(weight = "bold")
      ),
      locations = c(
        list(
          cells_column_labels(c("k", model.names)),
          cells_title("title"),
          cells_body(columns = "k")
        ),
        cells
      )
    )
}

model.names <- c("AIC", "BIC", "50/50-SS", "70/30-SS", "30/70-SS", "5-Fold-CV", "10-Fold-CV", "Wasserstein")

small.accuracy.table <- make.accuracy.table(small.result$best_k, k_max)
large.accuracy.table <- make.accuracy.table(large.result$best_k, k_max)

show.table(small.accuracy.table, "Model Accuracy (n=100, M=100)")
show.table(large.accuracy.table, "Model Accuracy (n=1000, M=100)")
```

Here are a few observations from the tables above and potential explanation of the results:

1. On the small sample each method seems a low number of features as expected, in fact as seen in the histogram plot earlier the distribution doesn't look like a mixture of gaussian or at best of 2.

2. On the large sample most models tend to favour a more complex model to have generated the data, a possible explanation for this is that with a higher number of features than the real one at least on a few examples it appears the EM-fit produces a better fit for the model. Though a $k>8$ has not been tested it is possible we would have kept getting better scores though with a diminishing difference wrt to a lower number of features.

```{r warning=FALSE}
#plotting function
plot_fit <- function(y, p, mu, sigma, xlab = "EM-fit for k=6", breaks = 50) {
  # sampling plot colors
  color_list <- grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]
  k    <- length(p)
  cols <- sample(color_list, k)
  
  hist(y, prob = T, breaks = breaks, col = gray(.8), border = NA, 
       main = "", xlab = xlab)
  set.seed(123)
  # compute max component for each obs
  norms <- vect_norm(y, mu, sigma)
  cols_idxs <- apply(norms, 1, which.max)
  curve_fun <- function(x) sum(vect_wnorm(x, p, mu, sigma))
  
  points(jitter(y), rep(0,length(y)), 
         pch = 19, cex = .6, 
         col = cols[cols_idxs])
  curve(sapply(x, curve_fun),
        lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
}


# generate a fit for k=7
XX <- sample_sim(1000)
params <- init_kmeans(XX, 6)
hm.fit <- handmade.em(XX, params$p, params$mu, params$sigma)
plot_fit(XX, hm.fit$p, hm.fit$mu, hm.fit$sigma)

# plot curve for k = 7
params <- init_kmeans(XX, 7)
hm.fit <- handmade.em(XX, params$p, params$mu, params$sigma)
plot_fit(XX, hm.fit$p, hm.fit$mu, hm.fit$sigma, xlab = "EM-fit for k=7")

#plot curve for k = 8
params <- init_kmeans(XX, 8)
hm.fit <- handmade.em(XX, params$p, params$mu, params$sigma)
plot_fit(XX, hm.fit$p, hm.fit$mu, hm.fit$sigma, xlab = "EM-fit for k=8")
```

3. Between the two sample split the 50-50 seems to perform slightly better while the 70-30 favours a more complex model and has a quite lower accuracy on $k^*$. The 30-70 seems to be suggest a choice of $k^*$ almost as frequently as the 50-50, maybe suggesting that using too much data on the training set leads to overfitting hence a more complex model.

4. The result of the BIC on the large sample are not clear to me as I expected the BIC to penalize complex models, as well as the AIC, but both seems to almost always predict a $k$ higher than the real.